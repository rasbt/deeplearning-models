{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEBilEjLj5wY"
   },
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 536,
     "status": "ok",
     "timestamp": 1524974472601,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "GOzuY8Yvj5wb",
    "outputId": "c19362ce-f87a-4cc2-84cc-8d7b4b9e6007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.6.8\n",
      "IPython 7.2.0\n",
      "\n",
<<<<<<< HEAD
      "torch 1.0.0\n"
=======
      "torch 1.0.1.post2\n"
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rH4XmErYj5wm"
   },
   "source": [
    "# Model Zoo -- Network in Network CIFAR-10 Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "### Network Architecture"
=======
    "based on \n",
    "\n",
    "- Lin, Min, Qiang Chen, and Shuicheng Yan. \"Network in network.\" arXiv preprint arXiv:1312.4400 (2013)."
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MkoGLH_Tj5wn"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ORj09gnrj5wp"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I6hghKPxj5w0"
   },
   "source": [
    "## Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 23936,
     "status": "ok",
     "timestamp": 1524974497505,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "NnT0sZIwj5wu",
    "outputId": "55aed925-d17e-4c6a-8c71-0d9b3bde5637"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "RANDOM_SEED = 1\n",
<<<<<<< HEAD
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 10\n",
=======
    "LEARNING_RATE = 0.0001\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 50\n",
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Other\n",
    "DEVICE = \"cuda:0\"\n",
    "GRAYSCALE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell that implements the ResNet-34 architecture is a derivative of the code provided at https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": 4,
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Image batch dimensions: torch.Size([256, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([256])\n",
      "Image batch dimensions: torch.Size([256, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### CIFAR-10 Dataset\n",
    "##########################\n",
    "\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          num_workers=8,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         num_workers=8,\n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 5,
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "### MODEL\n",
    "##########################\n",
    "\n",
    "\n",
    "class NiN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(NiN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.classifier = nn.Sequential(\n",
    "                nn.Conv2d(3, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 160, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(160,  96, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                nn.Dropout(0.5),\n",
    "\n",
    "                nn.Conv2d(96, 192, kernel_size=5, stride=1, padding=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=3, stride=2, padding=1),\n",
    "                nn.Dropout(0.5),\n",
    "\n",
    "                nn.Conv2d(192, 192, kernel_size=3, stride=1, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192, 192, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(192,  10, kernel_size=1, stride=1, padding=0),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.AvgPool2d(kernel_size=8, stride=1, padding=0),\n",
    "\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        logits = x.view(x.size(0), self.num_classes)\n",
<<<<<<< HEAD
    "        probas = torch.softmax(x, dim=1)\n",
=======
    "        probas = torch.softmax(logits, dim=1)\n",
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
    "        return logits, probas"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RAodboScj5w6"
   },
   "source": [
    "## Training without Pinned Memory"
   ]
  },
  {
=======
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_lza9t_uj5w1"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = NiN(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384585,
     "status": "ok",
     "timestamp": 1524976888520,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "Dzh3ROmRj5w7",
    "outputId": "5f8fd8c9-b076-403a-b0b7-fd2d498b48d7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Epoch: 001/010 | Batch 0000/0196 | Cost: 2.6021\n",
      "Epoch: 001/010 | Batch 0150/0196 | Cost: 1.3961\n",
      "Epoch: 001/010 | Train: 45.084%\n",
      "Time elapsed: 0.26 min\n",
      "Epoch: 002/010 | Batch 0000/0196 | Cost: 1.1228\n",
      "Epoch: 002/010 | Batch 0150/0196 | Cost: 1.0426\n",
      "Epoch: 002/010 | Train: 56.166%\n",
      "Time elapsed: 0.52 min\n",
      "Epoch: 003/010 | Batch 0000/0196 | Cost: 0.9980\n",
      "Epoch: 003/010 | Batch 0150/0196 | Cost: 0.8279\n",
      "Epoch: 003/010 | Train: 66.702%\n",
      "Time elapsed: 0.80 min\n",
      "Epoch: 004/010 | Batch 0000/0196 | Cost: 0.6384\n",
      "Epoch: 004/010 | Batch 0150/0196 | Cost: 0.7103\n",
      "Epoch: 004/010 | Train: 65.330%\n",
      "Time elapsed: 1.08 min\n",
      "Epoch: 005/010 | Batch 0000/0196 | Cost: 0.6308\n",
      "Epoch: 005/010 | Batch 0150/0196 | Cost: 0.5913\n",
      "Epoch: 005/010 | Train: 79.636%\n",
      "Time elapsed: 1.36 min\n",
      "Epoch: 006/010 | Batch 0000/0196 | Cost: 0.4409\n",
      "Epoch: 006/010 | Batch 0150/0196 | Cost: 0.5557\n",
      "Epoch: 006/010 | Train: 76.456%\n",
      "Time elapsed: 1.62 min\n",
      "Epoch: 007/010 | Batch 0000/0196 | Cost: 0.4778\n",
      "Epoch: 007/010 | Batch 0150/0196 | Cost: 0.4815\n",
      "Epoch: 007/010 | Train: 65.890%\n",
      "Time elapsed: 1.89 min\n",
      "Epoch: 008/010 | Batch 0000/0196 | Cost: 0.3782\n",
      "Epoch: 008/010 | Batch 0150/0196 | Cost: 0.4339\n",
      "Epoch: 008/010 | Train: 85.200%\n",
      "Time elapsed: 2.16 min\n",
      "Epoch: 009/010 | Batch 0000/0196 | Cost: 0.3083\n",
      "Epoch: 009/010 | Batch 0150/0196 | Cost: 0.3290\n",
      "Epoch: 009/010 | Train: 78.108%\n",
      "Time elapsed: 2.42 min\n",
      "Epoch: 010/010 | Batch 0000/0196 | Cost: 0.2229\n",
      "Epoch: 010/010 | Batch 0150/0196 | Cost: 0.1945\n",
      "Epoch: 010/010 | Train: 87.384%\n",
      "Time elapsed: 2.70 min\n",
      "Total Training Time: 2.70 min\n",
      "Test accuracy: 70.67%\n",
      "Total Time: 2.71 min\n"
=======
      "Epoch: 001/050 | Batch 0000/0196 | Cost: 2.3002\n",
      "Epoch: 001/050 | Batch 0150/0196 | Cost: 2.0776\n",
      "Epoch: 001/050 | Train: 23.810%\n",
      "Time elapsed: 0.32 min\n",
      "Epoch: 002/050 | Batch 0000/0196 | Cost: 1.9796\n",
      "Epoch: 002/050 | Batch 0150/0196 | Cost: 1.8938\n",
      "Epoch: 002/050 | Train: 27.458%\n",
      "Time elapsed: 0.64 min\n",
      "Epoch: 003/050 | Batch 0000/0196 | Cost: 1.9004\n",
      "Epoch: 003/050 | Batch 0150/0196 | Cost: 1.7400\n",
      "Epoch: 003/050 | Train: 30.426%\n",
      "Time elapsed: 0.97 min\n",
      "Epoch: 004/050 | Batch 0000/0196 | Cost: 1.8689\n",
      "Epoch: 004/050 | Batch 0150/0196 | Cost: 1.6299\n",
      "Epoch: 004/050 | Train: 35.662%\n",
      "Time elapsed: 1.32 min\n",
      "Epoch: 005/050 | Batch 0000/0196 | Cost: 1.6710\n",
      "Epoch: 005/050 | Batch 0150/0196 | Cost: 1.7306\n",
      "Epoch: 005/050 | Train: 36.898%\n",
      "Time elapsed: 1.66 min\n",
      "Epoch: 006/050 | Batch 0000/0196 | Cost: 1.5936\n",
      "Epoch: 006/050 | Batch 0150/0196 | Cost: 1.7595\n",
      "Epoch: 006/050 | Train: 37.268%\n",
      "Time elapsed: 2.00 min\n",
      "Epoch: 007/050 | Batch 0000/0196 | Cost: 1.6505\n",
      "Epoch: 007/050 | Batch 0150/0196 | Cost: 1.6429\n",
      "Epoch: 007/050 | Train: 39.406%\n",
      "Time elapsed: 2.35 min\n",
      "Epoch: 008/050 | Batch 0000/0196 | Cost: 1.5671\n",
      "Epoch: 008/050 | Batch 0150/0196 | Cost: 1.5692\n",
      "Epoch: 008/050 | Train: 41.852%\n",
      "Time elapsed: 2.68 min\n",
      "Epoch: 009/050 | Batch 0000/0196 | Cost: 1.6423\n",
      "Epoch: 009/050 | Batch 0150/0196 | Cost: 1.5157\n",
      "Epoch: 009/050 | Train: 42.832%\n",
      "Time elapsed: 3.02 min\n",
      "Epoch: 010/050 | Batch 0000/0196 | Cost: 1.5104\n",
      "Epoch: 010/050 | Batch 0150/0196 | Cost: 1.4086\n",
      "Epoch: 010/050 | Train: 44.496%\n",
      "Time elapsed: 3.36 min\n",
      "Epoch: 011/050 | Batch 0000/0196 | Cost: 1.6345\n",
      "Epoch: 011/050 | Batch 0150/0196 | Cost: 1.4711\n",
      "Epoch: 011/050 | Train: 44.582%\n",
      "Time elapsed: 3.70 min\n",
      "Epoch: 012/050 | Batch 0000/0196 | Cost: 1.5480\n",
      "Epoch: 012/050 | Batch 0150/0196 | Cost: 1.5178\n",
      "Epoch: 012/050 | Train: 46.584%\n",
      "Time elapsed: 4.04 min\n",
      "Epoch: 013/050 | Batch 0000/0196 | Cost: 1.4267\n",
      "Epoch: 013/050 | Batch 0150/0196 | Cost: 1.4330\n",
      "Epoch: 013/050 | Train: 46.268%\n",
      "Time elapsed: 4.38 min\n",
      "Epoch: 014/050 | Batch 0000/0196 | Cost: 1.4276\n",
      "Epoch: 014/050 | Batch 0150/0196 | Cost: 1.3600\n",
      "Epoch: 014/050 | Train: 47.486%\n",
      "Time elapsed: 4.72 min\n",
      "Epoch: 015/050 | Batch 0000/0196 | Cost: 1.3050\n",
      "Epoch: 015/050 | Batch 0150/0196 | Cost: 1.3514\n",
      "Epoch: 015/050 | Train: 47.986%\n",
      "Time elapsed: 5.06 min\n",
      "Epoch: 016/050 | Batch 0000/0196 | Cost: 1.5120\n",
      "Epoch: 016/050 | Batch 0150/0196 | Cost: 1.3317\n",
      "Epoch: 016/050 | Train: 48.294%\n",
      "Time elapsed: 5.40 min\n",
      "Epoch: 017/050 | Batch 0000/0196 | Cost: 1.5232\n",
      "Epoch: 017/050 | Batch 0150/0196 | Cost: 1.3222\n",
      "Epoch: 017/050 | Train: 49.158%\n",
      "Time elapsed: 5.74 min\n",
      "Epoch: 018/050 | Batch 0000/0196 | Cost: 1.4450\n",
      "Epoch: 018/050 | Batch 0150/0196 | Cost: 1.3963\n",
      "Epoch: 018/050 | Train: 49.628%\n",
      "Time elapsed: 6.08 min\n",
      "Epoch: 019/050 | Batch 0000/0196 | Cost: 1.3930\n",
      "Epoch: 019/050 | Batch 0150/0196 | Cost: 1.4259\n",
      "Epoch: 019/050 | Train: 50.140%\n",
      "Time elapsed: 6.42 min\n",
      "Epoch: 020/050 | Batch 0000/0196 | Cost: 1.5090\n",
      "Epoch: 020/050 | Batch 0150/0196 | Cost: 1.2891\n",
      "Epoch: 020/050 | Train: 51.298%\n",
      "Time elapsed: 6.76 min\n",
      "Epoch: 021/050 | Batch 0000/0196 | Cost: 1.3845\n",
      "Epoch: 021/050 | Batch 0150/0196 | Cost: 1.3206\n",
      "Epoch: 021/050 | Train: 50.642%\n",
      "Time elapsed: 7.10 min\n",
      "Epoch: 022/050 | Batch 0000/0196 | Cost: 1.4059\n",
      "Epoch: 022/050 | Batch 0150/0196 | Cost: 1.4076\n",
      "Epoch: 022/050 | Train: 52.204%\n",
      "Time elapsed: 7.44 min\n",
      "Epoch: 023/050 | Batch 0000/0196 | Cost: 1.2708\n",
      "Epoch: 023/050 | Batch 0150/0196 | Cost: 1.2555\n",
      "Epoch: 023/050 | Train: 52.876%\n",
      "Time elapsed: 7.78 min\n",
      "Epoch: 024/050 | Batch 0000/0196 | Cost: 1.1975\n",
      "Epoch: 024/050 | Batch 0150/0196 | Cost: 1.2196\n",
      "Epoch: 024/050 | Train: 53.576%\n",
      "Time elapsed: 8.12 min\n",
      "Epoch: 025/050 | Batch 0000/0196 | Cost: 1.2942\n",
      "Epoch: 025/050 | Batch 0150/0196 | Cost: 1.2967\n",
      "Epoch: 025/050 | Train: 53.844%\n",
      "Time elapsed: 8.46 min\n",
      "Epoch: 026/050 | Batch 0000/0196 | Cost: 1.2837\n",
      "Epoch: 026/050 | Batch 0150/0196 | Cost: 1.3414\n",
      "Epoch: 026/050 | Train: 54.562%\n",
      "Time elapsed: 8.81 min\n",
      "Epoch: 027/050 | Batch 0000/0196 | Cost: 1.2773\n",
      "Epoch: 027/050 | Batch 0150/0196 | Cost: 1.2936\n",
      "Epoch: 027/050 | Train: 53.750%\n",
      "Time elapsed: 9.14 min\n",
      "Epoch: 028/050 | Batch 0000/0196 | Cost: 1.2488\n",
      "Epoch: 028/050 | Batch 0150/0196 | Cost: 1.2489\n",
      "Epoch: 028/050 | Train: 55.114%\n",
      "Time elapsed: 9.48 min\n",
      "Epoch: 029/050 | Batch 0000/0196 | Cost: 1.2550\n",
      "Epoch: 029/050 | Batch 0150/0196 | Cost: 1.2256\n",
      "Epoch: 029/050 | Train: 53.806%\n",
      "Time elapsed: 9.82 min\n",
      "Epoch: 030/050 | Batch 0000/0196 | Cost: 1.2776\n",
      "Epoch: 030/050 | Batch 0150/0196 | Cost: 1.3692\n",
      "Epoch: 030/050 | Train: 56.104%\n",
      "Time elapsed: 10.16 min\n",
      "Epoch: 031/050 | Batch 0000/0196 | Cost: 1.2542\n",
      "Epoch: 031/050 | Batch 0150/0196 | Cost: 1.2809\n",
      "Epoch: 031/050 | Train: 56.256%\n",
      "Time elapsed: 10.50 min\n",
      "Epoch: 032/050 | Batch 0000/0196 | Cost: 1.1649\n",
      "Epoch: 032/050 | Batch 0150/0196 | Cost: 1.2762\n",
      "Epoch: 032/050 | Train: 55.820%\n",
      "Time elapsed: 10.84 min\n",
      "Epoch: 033/050 | Batch 0000/0196 | Cost: 1.2666\n",
      "Epoch: 033/050 | Batch 0150/0196 | Cost: 1.2294\n",
      "Epoch: 033/050 | Train: 55.314%\n",
      "Time elapsed: 11.18 min\n",
      "Epoch: 034/050 | Batch 0000/0196 | Cost: 1.1542\n",
      "Epoch: 034/050 | Batch 0150/0196 | Cost: 1.2219\n",
      "Epoch: 034/050 | Train: 57.514%\n",
      "Time elapsed: 11.52 min\n",
      "Epoch: 035/050 | Batch 0000/0196 | Cost: 1.1006\n",
      "Epoch: 035/050 | Batch 0150/0196 | Cost: 1.2530\n",
      "Epoch: 035/050 | Train: 57.442%\n",
      "Time elapsed: 11.86 min\n",
      "Epoch: 036/050 | Batch 0000/0196 | Cost: 1.1612\n",
      "Epoch: 036/050 | Batch 0150/0196 | Cost: 1.1750\n",
      "Epoch: 036/050 | Train: 57.866%\n",
      "Time elapsed: 12.19 min\n",
      "Epoch: 037/050 | Batch 0000/0196 | Cost: 1.2728\n",
      "Epoch: 037/050 | Batch 0150/0196 | Cost: 1.2318\n",
      "Epoch: 037/050 | Train: 57.824%\n",
      "Time elapsed: 12.53 min\n",
      "Epoch: 038/050 | Batch 0000/0196 | Cost: 1.2060\n",
      "Epoch: 038/050 | Batch 0150/0196 | Cost: 1.2510\n",
      "Epoch: 038/050 | Train: 56.996%\n",
      "Time elapsed: 12.87 min\n",
      "Epoch: 039/050 | Batch 0000/0196 | Cost: 1.0534\n",
      "Epoch: 039/050 | Batch 0150/0196 | Cost: 1.2304\n",
      "Epoch: 039/050 | Train: 59.268%\n",
      "Time elapsed: 13.21 min\n",
      "Epoch: 040/050 | Batch 0000/0196 | Cost: 1.1333\n",
      "Epoch: 040/050 | Batch 0150/0196 | Cost: 1.0861\n",
      "Epoch: 040/050 | Train: 59.360%\n",
      "Time elapsed: 13.55 min\n",
      "Epoch: 041/050 | Batch 0000/0196 | Cost: 1.1318\n",
      "Epoch: 041/050 | Batch 0150/0196 | Cost: 1.1789\n",
      "Epoch: 041/050 | Train: 60.284%\n",
      "Time elapsed: 13.89 min\n",
      "Epoch: 042/050 | Batch 0000/0196 | Cost: 1.1104\n",
      "Epoch: 042/050 | Batch 0150/0196 | Cost: 1.2120\n",
      "Epoch: 042/050 | Train: 59.580%\n",
      "Time elapsed: 14.23 min\n",
      "Epoch: 043/050 | Batch 0000/0196 | Cost: 1.1216\n",
      "Epoch: 043/050 | Batch 0150/0196 | Cost: 1.1255\n",
      "Epoch: 043/050 | Train: 59.894%\n",
      "Time elapsed: 14.57 min\n",
      "Epoch: 044/050 | Batch 0000/0196 | Cost: 1.1344\n",
      "Epoch: 044/050 | Batch 0150/0196 | Cost: 1.2295\n",
      "Epoch: 044/050 | Train: 60.898%\n",
      "Time elapsed: 14.91 min\n",
      "Epoch: 045/050 | Batch 0000/0196 | Cost: 1.1358\n",
      "Epoch: 045/050 | Batch 0150/0196 | Cost: 1.0987\n",
      "Epoch: 045/050 | Train: 61.258%\n",
      "Time elapsed: 15.25 min\n",
      "Epoch: 046/050 | Batch 0000/0196 | Cost: 1.1416\n",
      "Epoch: 046/050 | Batch 0150/0196 | Cost: 1.1000\n",
      "Epoch: 046/050 | Train: 60.392%\n",
      "Time elapsed: 15.58 min\n",
      "Epoch: 047/050 | Batch 0000/0196 | Cost: 1.2154\n",
      "Epoch: 047/050 | Batch 0150/0196 | Cost: 1.1993\n",
      "Epoch: 047/050 | Train: 59.848%\n",
      "Time elapsed: 15.93 min\n",
      "Epoch: 048/050 | Batch 0000/0196 | Cost: 1.1209\n",
      "Epoch: 048/050 | Batch 0150/0196 | Cost: 1.1330\n",
      "Epoch: 048/050 | Train: 62.328%\n",
      "Time elapsed: 16.26 min\n",
      "Epoch: 049/050 | Batch 0000/0196 | Cost: 1.0537\n",
      "Epoch: 049/050 | Batch 0150/0196 | Cost: 1.0301\n",
      "Epoch: 049/050 | Train: 61.910%\n",
      "Time elapsed: 16.60 min\n",
      "Epoch: 050/050 | Batch 0000/0196 | Cost: 1.0365\n",
      "Epoch: 050/050 | Batch 0150/0196 | Cost: 1.1937\n",
      "Epoch: 050/050 | Train: 62.252%\n",
      "Time elapsed: 16.94 min\n",
      "Total Training Time: 16.94 min\n",
      "Test accuracy: 60.96%\n",
      "Total Time: 16.96 min\n"
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 150:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE)))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))\n",
    "    \n",
    "print('Total Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
<<<<<<< HEAD
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Pinned Memory"
   ]
  },
  {
=======
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Files already downloaded and verified\n",
      "Image batch dimensions: torch.Size([256, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([256])\n",
      "Image batch dimensions: torch.Size([256, 3, 32, 32])\n",
      "Image label dimensions: torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### CIFAR-10 Dataset\n",
    "##########################\n",
    "\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.CIFAR10(root='data', \n",
    "                                 train=True, \n",
    "                                 transform=transforms.ToTensor(),\n",
    "                                 download=True)\n",
    "\n",
    "test_dataset = datasets.CIFAR10(root='data', \n",
    "                                train=False, \n",
    "                                transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          pin_memory=True,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         pin_memory=True,\n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_lza9t_uj5w1"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "model = resnet34(NUM_CLASSES)\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2384585,
     "status": "ok",
     "timestamp": 1524976888520,
     "user": {
      "displayName": "Sebastian Raschka",
      "photoUrl": "//lh6.googleusercontent.com/-cxK6yOSQ6uE/AAAAAAAAAAI/AAAAAAAAIfw/P9ar_CHsKOQ/s50-c-k-no/photo.jpg",
      "userId": "118404394130788869227"
     },
     "user_tz": 240
    },
    "id": "Dzh3ROmRj5w7",
    "outputId": "5f8fd8c9-b076-403a-b0b7-fd2d498b48d7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Batch 0000/0196 | Cost: 2.6021\n",
      "Epoch: 001/010 | Batch 0150/0196 | Cost: 1.3961\n",
      "Epoch: 001/010 | Train: 45.084%\n",
      "Time elapsed: 0.39 min\n",
      "Epoch: 002/010 | Batch 0000/0196 | Cost: 1.1228\n",
      "Epoch: 002/010 | Batch 0150/0196 | Cost: 1.0426\n",
      "Epoch: 002/010 | Train: 56.166%\n",
      "Time elapsed: 0.77 min\n",
      "Epoch: 003/010 | Batch 0000/0196 | Cost: 0.9980\n",
      "Epoch: 003/010 | Batch 0150/0196 | Cost: 0.8279\n",
      "Epoch: 003/010 | Train: 66.702%\n",
      "Time elapsed: 1.16 min\n",
      "Epoch: 004/010 | Batch 0000/0196 | Cost: 0.6384\n",
      "Epoch: 004/010 | Batch 0150/0196 | Cost: 0.7103\n",
      "Epoch: 004/010 | Train: 65.330%\n",
      "Time elapsed: 1.55 min\n",
      "Epoch: 005/010 | Batch 0000/0196 | Cost: 0.6308\n",
      "Epoch: 005/010 | Batch 0150/0196 | Cost: 0.5913\n",
      "Epoch: 005/010 | Train: 79.636%\n",
      "Time elapsed: 1.94 min\n",
      "Epoch: 006/010 | Batch 0000/0196 | Cost: 0.4409\n",
      "Epoch: 006/010 | Batch 0150/0196 | Cost: 0.5557\n",
      "Epoch: 006/010 | Train: 76.456%\n",
      "Time elapsed: 2.33 min\n",
      "Epoch: 007/010 | Batch 0000/0196 | Cost: 0.4778\n",
      "Epoch: 007/010 | Batch 0150/0196 | Cost: 0.4815\n",
      "Epoch: 007/010 | Train: 65.890%\n",
      "Time elapsed: 2.71 min\n",
      "Epoch: 008/010 | Batch 0000/0196 | Cost: 0.3782\n",
      "Epoch: 008/010 | Batch 0150/0196 | Cost: 0.4339\n",
      "Epoch: 008/010 | Train: 85.200%\n",
      "Time elapsed: 3.10 min\n",
      "Epoch: 009/010 | Batch 0000/0196 | Cost: 0.3083\n",
      "Epoch: 009/010 | Batch 0150/0196 | Cost: 0.3290\n",
      "Epoch: 009/010 | Train: 78.108%\n",
      "Time elapsed: 3.49 min\n",
      "Epoch: 010/010 | Batch 0000/0196 | Cost: 0.2229\n",
      "Epoch: 010/010 | Batch 0150/0196 | Cost: 0.1945\n",
      "Epoch: 010/010 | Train: 87.384%\n",
      "Time elapsed: 3.88 min\n",
      "Total Training Time: 3.88 min\n",
      "Test accuracy: 70.67%\n",
      "Total Time: 3.91 min\n"
     ]
    }
   ],
   "source": [
    "def compute_accuracy(model, data_loader, device):\n",
    "    correct_pred, num_examples = 0, 0\n",
    "    for i, (features, targets) in enumerate(data_loader):\n",
    "            \n",
    "        features = features.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        logits, probas = model(features)\n",
    "        _, predicted_labels = torch.max(probas, 1)\n",
    "        num_examples += targets.size(0)\n",
    "        correct_pred += (predicted_labels == targets).sum()\n",
    "    return correct_pred.float()/num_examples * 100\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    for batch_idx, (features, targets) in enumerate(train_loader):\n",
    "        \n",
    "        features = features.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "            \n",
    "        ### FORWARD AND BACK PROP\n",
    "        logits, probas = model(features)\n",
    "        cost = F.cross_entropy(logits, targets)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        cost.backward()\n",
    "        \n",
    "        ### UPDATE MODEL PARAMETERS\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### LOGGING\n",
    "        if not batch_idx % 150:\n",
    "            print ('Epoch: %03d/%03d | Batch %04d/%04d | Cost: %.4f' \n",
    "                   %(epoch+1, NUM_EPOCHS, batch_idx, \n",
    "                     len(train_loader), cost))\n",
    "\n",
    "        \n",
    "\n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False): # save memory during inference\n",
    "        print('Epoch: %03d/%03d | Train: %.3f%%' % (\n",
    "              epoch+1, NUM_EPOCHS, \n",
    "              compute_accuracy(model, train_loader, device=DEVICE)))\n",
    "        \n",
    "    print('Time elapsed: %.2f min' % ((time.time() - start_time)/60))\n",
    "    \n",
    "print('Total Training Time: %.2f min' % ((time.time() - start_time)/60))\n",
    "\n",
    "\n",
    "with torch.set_grad_enabled(False): # save memory during inference\n",
    "    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))\n",
    "    \n",
    "print('Total Time: %.2f min' % ((time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the training time without and with `pin_memory=True`, there doesn't seem to be a speed-up when using page-locked (or \"pinned\") memory -- in fact, pinning the memory even slowed down the training. (I reran the code in the opposite order, i.e., `pin_memory=True` first, and got the same results.) This could be due to the relatively small dataset size, batch size, and hardware configuration that I was using:\n",
    "\n",
    "- Processor: Intel XeonÂ® Processor E5-2650 v4 (12 core)\n",
    "- GPU: NVIDIA GeForce GTX 1080Ti\n",
    "- Motherboard: ASUS X99-E-10G WS with PCI-E Gen3 X16 port\n",
    "- Memory: 128 GB DDR4 RAM\n",
    "- Storage: SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy       1.15.4\n",
      "pandas      0.23.4\n",
      "torch       1.0.0\n",
=======
      "numpy       1.15.4\n",
      "pandas      0.23.4\n",
      "torch       1.0.1.post2\n",
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
      "PIL.Image   5.3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "convnet-vgg16.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.7.1"
=======
   "version": "3.6.8"
>>>>>>> 508262462e2897612b7ef55f50b9b9e5ebc15550
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "371px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
