{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Accompanying code examples of the book \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" by [Sebastian Raschka](https://sebastianraschka.com). All code examples are released under the [MIT license](https://github.com/rasbt/deep-learning-book/blob/master/LICENSE). If you find this content useful, please consider supporting the work by buying a [copy of the book](https://leanpub.com/ann-and-deeplearning).*\n",
    "  \n",
    "Other code examples and content are available on [GitHub](https://github.com/rasbt/deep-learning-book). The PDF and ebook versions of the book are available through [Leanpub](https://leanpub.com/ann-and-deeplearning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Raschka \n",
      "\n",
      "CPython 3.7.1\n",
      "IPython 7.2.0\n",
      "\n",
      "torch 1.0.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -v -p torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo -- Using PyTorch Dataset Loading Utilities for Custom Datasets (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides an example for how to load an image dataset, stored as individual PNG files, using PyTorch's data loading utilities. For a more in-depth discussion, please see the official\n",
    "\n",
    "- [Data Loading and Processing Tutorial](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html)\n",
    "- [torch.utils.data](http://pytorch.org/docs/master/data.html) API documentation\n",
    "\n",
    "In this example, we are using the cropped version of the **Street View House Numbers (SVHN) Dataset**, which is available at http://ufldl.stanford.edu/housenumbers/. \n",
    "\n",
    "To execute the following examples, you need to download the 2 \".mat\" files \n",
    "\n",
    "- [train_32x32.mat](http://ufldl.stanford.edu/housenumbers/train_32x32.mat) (ca. 182 Mb, 73,257 images)\n",
    "- [test_32x32.mat](http://ufldl.stanford.edu/housenumbers/test_32x32.mat) (ca. 65 Mb, 26,032 images)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.io as sio\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will convert the images from \".mat\" into individual \".png\" files. In addition, we will create CSV contained the image paths and associated class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pngs(main_dir, mat_file, label):\n",
    "    \n",
    "    if not os.path.exists(main_dir):\n",
    "        os.mkdir(main_dir)\n",
    "        \n",
    "    sub_dir = os.path.join(main_dir, label)\n",
    "    if not os.path.exists(sub_dir):\n",
    "        os.mkdir(sub_dir)\n",
    "\n",
    "    data = sio.loadmat(mat_file)\n",
    "\n",
    "    X = np.transpose(data['X'], (3, 0, 1, 2))\n",
    "    y = data['y'].flatten()\n",
    "\n",
    "    with open(os.path.join(main_dir, '%s_labels.csv' % label), 'w') as out_f:\n",
    "        for i, img in enumerate(X):\n",
    "            file_path = os.path.join(sub_dir, str(i) + '.png')\n",
    "            imageio.imwrite(os.path.join(file_path),\n",
    "                            img)\n",
    "\n",
    "            out_f.write(\"%d.png,%d\\n\" % (i, y[i]))\n",
    "\n",
    "            \n",
    "make_pngs(main_dir='svhn_cropped',\n",
    "          mat_file='train_32x32.mat',\n",
    "          label='train')\n",
    "    \n",
    "    \n",
    "make_pngs(main_dir='svhn_cropped',\n",
    "          mat_file='test_32x32.mat',\n",
    "          label='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.png</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.png</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.png</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.png</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.png</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       1\n",
       "0       \n",
       "0.png  1\n",
       "1.png  9\n",
       "2.png  2\n",
       "3.png  3\n",
       "4.png  2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('svhn_cropped/train_labels.csv', header=None, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.png</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.png</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.png</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.png</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.png</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        1\n",
       "0        \n",
       "0.png   5\n",
       "1.png   2\n",
       "2.png   1\n",
       "3.png  10\n",
       "4.png   6"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('svhn_cropped/test_labels.csv', header=None, index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Custom Dataset Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we implement a custom `Dataset` for reading the images. The `__getitem__` method will\n",
    "\n",
    "1. read a single image from disk based on an `index` (more on batching later)\n",
    "2. perform a custom image transformation (if a `transform` argument is provided in the `__init__` construtor)\n",
    "3. return a single image and it's corresponding label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVHNDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for loading cropped SVHN images\"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path, img_dir, transform=None):\n",
    "    \n",
    "        df = pd.read_csv(csv_path, index_col=0, header=None)\n",
    "        self.img_dir = img_dir\n",
    "        self.csv_path = csv_path\n",
    "        self.img_names = df.index.values\n",
    "        self.y = df[1].values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(os.path.join(self.img_dir,\n",
    "                                      self.img_names[index]))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        label = self.y[index]\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created our custom Dataset class, let us add some custom transformations via the `transforms` utilities from `torchvision`, we\n",
    "\n",
    "1. normalize the images (here: dividing by 255)\n",
    "2. converting the image arrays into PyTorch tensors\n",
    "\n",
    "Then, we initialize a Dataset instance for the training images using the 'quickdraw_png_set1_train.csv' label file (we omit the test set, but the same concepts apply).\n",
    "\n",
    "Finally, we initialize a `DataLoader` that allows us to read from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that transforms.ToTensor()\n",
    "# already divides pixels by 255. internally\n",
    "\n",
    "custom_transform = transforms.Compose([#transforms.Grayscale(),                                       \n",
    "                                       #transforms.Lambda(lambda x: x/255.),\n",
    "                                       transforms.ToTensor()])\n",
    "\n",
    "train_dataset = SVHNDataset(csv_path='svhn_cropped/train_labels.csv',\n",
    "                              img_dir='svhn_cropped/train',\n",
    "                              transform=custom_transform)\n",
    "\n",
    "test_dataset = SVHNDataset(csv_path='svhn_cropped/test_labels.csv',\n",
    "                             img_dir='svhn_cropped/test',\n",
    "                             transform=custom_transform)\n",
    "\n",
    "BATCH_SIZE=128\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False,\n",
    "                         num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it, now we can iterate over an epoch using the train_loader as an iterator and use the features and labels from the training dataset for model training:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating Through the Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Batch index: 0 | Batch size: 128\n",
      "Epoch: 2 | Batch index: 0 | Batch size: 128\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "num_epochs = 2\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        \n",
    "        print('Epoch:', epoch+1, end='')\n",
    "        print(' | Batch index:', batch_idx, end='')\n",
    "        print(' | Batch size:', y.size()[0])\n",
    "        \n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure that the batches are being loaded correctly, let's print out the dimensions of the last batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 3, 32, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, each batch consists of 128 images, just as specified. However, one thing to keep in mind though is that\n",
    "PyTorch uses a different image layout (which is more efficient when working with CUDA); here, the image axes are \"num_images x channels x height x width\" (NCHW) instead of \"num_images height x width x channels\" (NHWC):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visually check that the images that coming of the data loader are intact, let's swap the axes to NHWC and convert an image from a Torch Tensor to a NumPy array so that we can visualize the image via `imshow`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 32, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_image = x[99].permute(1, 2, 0)\n",
    "one_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# note that imshow also works fine with scaled\n",
    "# images in [0, 1] range.\n",
    "plt.imshow(one_image.to(torch.device('cpu')));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas      0.23.4\n",
      "scipy       1.1.0\n",
      "matplotlib  3.0.2\n",
      "imageio     2.4.1\n",
      "torch       1.0.0\n",
      "PIL.Image   5.3.0\n",
      "numpy       1.15.4\n",
      "torchvision 0.2.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -iv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
